\section{Introduction}

In this work we are gonna present the solution for task of text
``detoxification''. The task is to present the solution which will paraphrase
(or otherwise modify) the given sentence, making it non-toxic and preserving
initial meaning. The approach we are going to present is fine-tuning of
encoder-decoder transformer architecture.

For instance, we choose pretrained T5~\cite{} transformer as base model. Then
we fine-tune it on a small selection of english paraphrasing corpora. Corpora
contains sentence pairs --- toxic sentence with non-toxic ideal paraphasing.

The purpose of this work is to demonstrate that it even small amount of data is
enough to help well pretrained general-purpose transformer capture specifics of
the task. Narrowing down the context of the model or teaching it the new style
(e.g.~non-toxic) can find a use in a wide variety of applications. As a result,
effective tuning of the model with small amount of resources can be incredible
advantage of such kind of models.

The quality of solution will be tested against with baseline algorithmic
solution. Moreover, solution will be compared with SOTA model, based on BART
architecture~\cite{}.