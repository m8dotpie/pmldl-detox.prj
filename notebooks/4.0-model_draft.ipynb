{"cells":[{"cell_type":"markdown","metadata":{"id":"rEJBSTyZIrIb"},"source":["# Practical machine learning and deep learning. Lab 5\n","## Competition\n","No competition for today\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Fine-tuning a model on a translation task\n","Today we will be finetunning T5(Text-To-Text Transfer Transformer) [model](https://github.com/google-research/t5x) on translation task. For this purpose we will be using [HuggingFace transformers](https://huggingface.co/docs/transformers/index) and [WMT16](https://huggingface.co/datasets/wmt16) dataset. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"MOsHUjgdIrIW","outputId":"f84a093e-147f-470e-aad9-80fb51193c8e","trusted":true},"outputs":[],"source":["# installing huggingface libraries for dataset, models and metrics\n","!pip install datasets transformers[sentencepiece] sacrebleu\n","\n","!pip install numpy==1.24.3"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:01.766013Z","iopub.status.busy":"2023-09-24T15:10:01.765366Z","iopub.status.idle":"2023-09-24T15:10:01.772400Z","shell.execute_reply":"2023-09-24T15:10:01.771384Z","shell.execute_reply.started":"2023-09-24T15:10:01.765977Z"},"trusted":true},"outputs":[],"source":["# Necessary inputs\n","import warnings\n","\n","from datasets import load_dataset, load_metric\n","import transformers\n","import datasets\n","import random\n","import pandas as pd\n","from IPython.display import display, HTML\n","\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":["## Selecting the model\n","For the example purpose we select as model checkpoint the smallest transformer in T5 family - `t5_small`. Other pre-trained models can be found [here](https://huggingface.co/docs/transformers/model_doc/t5#:~:text=T5%20comes%20in%20different%20sizes%3A)."]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:01.774778Z","iopub.status.busy":"2023-09-24T15:10:01.773897Z","iopub.status.idle":"2023-09-24T15:10:01.784448Z","shell.execute_reply":"2023-09-24T15:10:01.783216Z","shell.execute_reply.started":"2023-09-24T15:10:01.774744Z"},"trusted":true},"outputs":[],"source":["# selecting model checkpoint\n","model_checkpoint = \"t5-small\""]},{"cell_type":"markdown","metadata":{"id":"whPRbBNbIrIl"},"source":["## Loading the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:01.787917Z","iopub.status.busy":"2023-09-24T15:10:01.787594Z","iopub.status.idle":"2023-09-24T15:10:03.219266Z","shell.execute_reply":"2023-09-24T15:10:03.218277Z","shell.execute_reply.started":"2023-09-24T15:10:01.787893Z"},"id":"IreSlFmlIrIm","trusted":true},"outputs":[],"source":["# setting random seed for transformers library\n","transformers.set_seed(42)\n","\n","# Load the WMT16 dataset\n","raw_datasets = load_dataset(\"wmt16\", \"de-en\")\n","\n","# Load the BLUE metric\n","metric = load_metric(\"sacrebleu\")"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset\n","Downloaded from HuggingFace dataset is a `DatasetDict`. It contains keys `[\"train\", \"validation\", \"test\"]` - which represents a dataset splits"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:03.220970Z","iopub.status.busy":"2023-09-24T15:10:03.220604Z","iopub.status.idle":"2023-09-24T15:10:03.229080Z","shell.execute_reply":"2023-09-24T15:10:03.228070Z","shell.execute_reply.started":"2023-09-24T15:10:03.220925Z"},"id":"GWiVUF0jIrIv","outputId":"35e3ea43-f397-4a54-c90c-f2cf8d36873e","trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['translation'],\n","        num_rows: 4548885\n","    })\n","    validation: Dataset({\n","        features: ['translation'],\n","        num_rows: 2169\n","    })\n","    test: Dataset({\n","        features: ['translation'],\n","        num_rows: 2999\n","    })\n","})"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["raw_datasets"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:35:02.763581Z","iopub.status.busy":"2023-09-24T15:35:02.762411Z","iopub.status.idle":"2023-09-24T15:35:02.772979Z","shell.execute_reply":"2023-09-24T15:35:02.771223Z","shell.execute_reply.started":"2023-09-24T15:35:02.763514Z"},"id":"X6HrpprwIrIz","outputId":"d7670bc0-42e4-4c09-8a6a-5c018ded7d95","trusted":true},"outputs":[{"data":{"text/plain":["{'translation': [{'de': 'Wiederaufnahme der Sitzungsperiode',\n","   'en': 'Resumption of the session'},\n","  {'de': 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.',\n","   'en': 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.'},\n","  {'de': 'Wie Sie feststellen konnten, ist der gefürchtete \"Millenium-Bug \" nicht eingetreten. Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden.',\n","   'en': \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\"},\n","  {'de': 'Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen.',\n","   'en': 'You have requested a debate on this subject in the course of the next few days, during this part-session.'},\n","  {'de': 'Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen -, allen Opfern der Stürme, insbesondere in den verschiedenen Ländern der Europäischen Union, in einer Schweigeminute zu gedenken.',\n","   'en': \"In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\"}]}"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["# samples from train dataset\n","raw_datasets[\"train\"][:5]"]},{"cell_type":"markdown","metadata":{},"source":["## Metric\n","[Sacrebleu](https://huggingface.co/spaces/evaluate-metric/sacrebleu) computes:\n","- `score`: BLEU score\n","- `counts`: list of counts of correct n-grams\n","- `totals`: list of counts of total n-grams\n","- `precisions`: list of precisions\n","- `bp`: Brevity penalty\n","- `sys_len`: cumulative sysem length\n","- `ref_len`: cumulative reference length\n","\n","The main metric is [BLEU score](https://en.wikipedia.org/wiki/BLEU). BLEU (BiLingual Evaluation Understudy) is a metric for automatically evaluating machine-translated text. The BLEU score measures the similarity of the machine-translated text to a set of high quality reference translations.\n","\n","The BLEU metric is calculates using [n-grams](https://en.wikipedia.org/wiki/N-gram)."]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:38:54.393583Z","iopub.status.busy":"2023-09-24T15:38:54.393131Z","iopub.status.idle":"2023-09-24T15:38:54.403154Z","shell.execute_reply":"2023-09-24T15:38:54.401952Z","shell.execute_reply.started":"2023-09-24T15:38:54.393549Z"},"id":"5o4rUteaIrI_","outputId":"18038ef5-554c-45c5-e00a-133b02ec10f1","trusted":true},"outputs":[{"data":{"text/plain":["Metric(name: \"sacrebleu\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, usage: \"\"\"\n","Produces BLEU scores along with its sufficient statistics\n","from a source against one or more references.\n","\n","Args:\n","    predictions (`list` of `str`): list of translations to score. Each translation should be tokenized into a list of tokens.\n","    references (`list` of `list` of `str`): A list of lists of references. The contents of the first sub-list are the references for the first prediction, the contents of the second sub-list are for the second prediction, etc. Note that there must be the same number of references for each prediction (i.e. all sub-lists must be of the same length).\n","    smooth_method (`str`): The smoothing method to use, defaults to `'exp'`. Possible values are:\n","        - `'none'`: no smoothing\n","        - `'floor'`: increment zero counts\n","        - `'add-k'`: increment num/denom by k for n>1\n","        - `'exp'`: exponential decay\n","    smooth_value (`float`): The smoothing value. Only valid when `smooth_method='floor'` (in which case `smooth_value` defaults to `0.1`) or `smooth_method='add-k'` (in which case `smooth_value` defaults to `1`).\n","    tokenize (`str`): Tokenization method to use for BLEU. If not provided, defaults to `'zh'` for Chinese, `'ja-mecab'` for Japanese and `'13a'` (mteval) otherwise. Possible values are:\n","        - `'none'`: No tokenization.\n","        - `'zh'`: Chinese tokenization.\n","        - `'13a'`: mimics the `mteval-v13a` script from Moses.\n","        - `'intl'`: International tokenization, mimics the `mteval-v14` script from Moses\n","        - `'char'`: Language-agnostic character-level tokenization.\n","        - `'ja-mecab'`: Japanese tokenization. Uses the [MeCab tokenizer](https://pypi.org/project/mecab-python3).\n","    lowercase (`bool`): If `True`, lowercases the input, enabling case-insensitivity. Defaults to `False`.\n","    force (`bool`): If `True`, insists that your tokenized input is actually detokenized. Defaults to `False`.\n","    use_effective_order (`bool`): If `True`, stops including n-gram orders for which precision is 0. This should be `True`, if sentence-level BLEU will be computed. Defaults to `False`.\n","\n","Returns:\n","    'score': BLEU score,\n","    'counts': Counts,\n","    'totals': Totals,\n","    'precisions': Precisions,\n","    'bp': Brevity penalty,\n","    'sys_len': predictions length,\n","    'ref_len': reference length,\n","\n","Examples:\n","\n","    Example 1:\n","        >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n","        >>> references = [[\"hello there general kenobi\", \"hello there !\"], [\"foo bar foobar\", \"foo bar foobar\"]]\n","        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n","        >>> results = sacrebleu.compute(predictions=predictions, references=references)\n","        >>> print(list(results.keys()))\n","        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n","        >>> print(round(results[\"score\"], 1))\n","        100.0\n","\n","    Example 2:\n","        >>> predictions = [\"hello there general kenobi\",\n","        ...                 \"on our way to ankh morpork\"]\n","        >>> references = [[\"hello there general kenobi\", \"hello there !\"],\n","        ...                 [\"goodbye ankh morpork\", \"ankh morpork\"]]\n","        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n","        >>> results = sacrebleu.compute(predictions=predictions,\n","        ...                             references=references)\n","        >>> print(list(results.keys()))\n","        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n","        >>> print(round(results[\"score\"], 1))\n","        39.8\n","\"\"\", stored examples: 0)"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["# how to use sacrebleu and its purpose\n","metric"]},{"cell_type":"code","execution_count":86,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T16:05:06.585010Z","iopub.status.busy":"2023-09-24T16:05:06.584619Z","iopub.status.idle":"2023-09-24T16:05:06.601506Z","shell.execute_reply":"2023-09-24T16:05:06.600239Z","shell.execute_reply.started":"2023-09-24T16:05:06.584982Z"},"id":"6XN1Rq0aIrJC","outputId":"a4405435-a8a9-41ff-9f79-a13077b587c7","trusted":true},"outputs":[{"data":{"text/plain":["{'score': 45.59274666224604,\n"," 'counts': [7, 4, 1, 0],\n"," 'totals': [9, 6, 3, 2],\n"," 'precisions': [77.77777777777777,\n","  66.66666666666667,\n","  33.333333333333336,\n","  25.0],\n"," 'bp': 1.0,\n"," 'sys_len': 9,\n"," 'ref_len': 9}"]},"execution_count":86,"metadata":{},"output_type":"execute_result"}],"source":["fake_preds = [\"hello there\", \"general kenobi\", \"Can I get an A\"]\n","fake_labels = [[\"hello there\"], [\"general kenobi\"], ['Can I get a C']]\n","metric.compute(predictions=fake_preds, references=fake_labels)"]},{"cell_type":"markdown","metadata":{"id":"n9qywopnIrJH"},"source":["## Preprocessing the data\n","As usual we will need to preprocess data and tokenize it before passing to model"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:03.310034Z","iopub.status.busy":"2023-09-24T15:10:03.309664Z","iopub.status.idle":"2023-09-24T15:10:03.505289Z","shell.execute_reply":"2023-09-24T15:10:03.504208Z","shell.execute_reply.started":"2023-09-24T15:10:03.310001Z"},"id":"eXNLu_-nIrJI","trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","# we will use autotokenizer for this purpose\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:03.507243Z","iopub.status.busy":"2023-09-24T15:10:03.506858Z","iopub.status.idle":"2023-09-24T15:10:03.514992Z","shell.execute_reply":"2023-09-24T15:10:03.513936Z","shell.execute_reply.started":"2023-09-24T15:10:03.507207Z"},"id":"a5hBlsrHIrJL","outputId":"acdaa98a-a8cd-4a20-89b8-cc26437bbe90","trusted":true},"outputs":[{"data":{"text/plain":["{'input_ids': [8774, 6, 48, 80, 7142, 55, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer(\"Hello, this one sentence!\")"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:03.517405Z","iopub.status.busy":"2023-09-24T15:10:03.516686Z","iopub.status.idle":"2023-09-24T15:10:03.529526Z","shell.execute_reply":"2023-09-24T15:10:03.528540Z","shell.execute_reply.started":"2023-09-24T15:10:03.517370Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'input_ids': [[8774, 6, 48, 80, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"])"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:03.533132Z","iopub.status.busy":"2023-09-24T15:10:03.532860Z","iopub.status.idle":"2023-09-24T15:10:03.539292Z","shell.execute_reply":"2023-09-24T15:10:03.538145Z","shell.execute_reply.started":"2023-09-24T15:10:03.533109Z"},"trusted":true},"outputs":[],"source":["# prefix for model input\n","prefix = \"translate English to Deutsch:\""]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:03.541473Z","iopub.status.busy":"2023-09-24T15:10:03.540621Z","iopub.status.idle":"2023-09-24T15:10:03.549770Z","shell.execute_reply":"2023-09-24T15:10:03.548874Z","shell.execute_reply.started":"2023-09-24T15:10:03.541440Z"},"id":"vc0BSBLIIrJQ","trusted":true},"outputs":[],"source":["max_input_length = 128\n","max_target_length = 128\n","source_lang = \"en\"\n","target_lang = \"de\"\n","\n","def preprocess_function(examples):\n","    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n","    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n","    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n","\n","    # Setup the tokenizer for targets\n","    labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:03.551620Z","iopub.status.busy":"2023-09-24T15:10:03.551106Z","iopub.status.idle":"2023-09-24T15:10:03.571019Z","shell.execute_reply":"2023-09-24T15:10:03.569879Z","shell.execute_reply.started":"2023-09-24T15:10:03.551587Z"},"id":"-b70jh26IrJS","outputId":"acd3a42d-985b-44ee-9daa-af5d944ce1d9","trusted":true},"outputs":[{"data":{"text/plain":["{'input_ids': [[13959, 1566, 12, 3, 18609, 10, 419, 4078, 102, 1575, 13, 8, 2363, 1], [13959, 1566, 12, 3, 18609, 10, 27, 15884, 4258, 26, 8, 2363, 13, 8, 1611, 12876, 19181, 1211, 29, 15, 26, 30, 1701, 1003, 1882, 5247, 6, 11, 27, 133, 114, 728, 541, 12, 1663, 25, 3, 9, 1095, 126, 215, 16, 8, 897, 24, 25, 2994, 3, 9, 8714, 15723, 1059, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[15158, 24860, 74, 11216, 425, 7, 4267, 32, 221, 1], [1674, 3, 49, 20635, 15, 67, 183, 17874, 6, 340, 11030, 17900, 1199, 5702, 1559, 15, 11216, 425, 7, 4267, 32, 221, 93, 3, 30604, 29, 13636, 7, 218, 1403, 3019, 7026, 6, 3, 25084, 2587, 18794, 7, 3532, 7756, 15, 674, 9242, 11621, 64, 3, 11950, 15, 6, 3, 26, 7118, 292, 11878, 16849, 8827, 5, 1]]}"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["# example of preprocessing\n","preprocess_function(raw_datasets['train'][:2])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T16:09:35.662567Z","iopub.status.busy":"2023-09-24T16:09:35.662099Z","iopub.status.idle":"2023-09-24T16:09:37.545512Z","shell.execute_reply":"2023-09-24T16:09:37.544516Z","shell.execute_reply.started":"2023-09-24T16:09:35.662533Z"},"id":"DDtsaJeVIrJT","outputId":"aa4734bf-4ef5-4437-9948-2c16363da719","trusted":true},"outputs":[],"source":["# for the example purpose we will crop the dataset and select first 5000 for train\n","# and 500 for validation and test\n","cropped_datasets = raw_datasets\n","cropped_datasets['train'] = raw_datasets['train'].select(range(5000))\n","cropped_datasets['validation'] = raw_datasets['validation'].select(range(500))\n","cropped_datasets['test'] = raw_datasets['test'].select(range(500))\n","tokenized_datasets = cropped_datasets.map(preprocess_function, batched=True)\n","tokenized_datasets['train'][0]"]},{"cell_type":"markdown","metadata":{"id":"545PP3o8IrJV"},"source":["## Fine-tuning the model"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:03.605060Z","iopub.status.busy":"2023-09-24T15:10:03.604730Z","iopub.status.idle":"2023-09-24T15:10:04.671893Z","shell.execute_reply":"2023-09-24T15:10:04.670859Z","shell.execute_reply.started":"2023-09-24T15:10:03.605029Z"},"id":"TlqNaB8jIrJW","outputId":"84916cf3-6e6c-47f3-d081-032ec30a4132","trusted":true},"outputs":[],"source":["from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","# create a model for the pretrained model\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:04.674163Z","iopub.status.busy":"2023-09-24T15:10:04.673474Z","iopub.status.idle":"2023-09-24T15:10:04.681771Z","shell.execute_reply":"2023-09-24T15:10:04.680562Z","shell.execute_reply.started":"2023-09-24T15:10:04.674126Z"},"id":"Bliy8zgjIrJY","trusted":true},"outputs":[],"source":["# defining the parameters for training\n","batch_size = 32\n","model_name = model_checkpoint.split(\"/\")[-1]\n","args = Seq2SeqTrainingArguments(\n","    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    weight_decay=0.01,\n","    save_total_limit=3,\n","    num_train_epochs=10,\n","    predict_with_generate=True,\n","    fp16=True,\n","    report_to='tensorboard',\n",")"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:04.684376Z","iopub.status.busy":"2023-09-24T15:10:04.683883Z","iopub.status.idle":"2023-09-24T15:10:04.693774Z","shell.execute_reply":"2023-09-24T15:10:04.692863Z","shell.execute_reply.started":"2023-09-24T15:10:04.684341Z"},"trusted":true},"outputs":[],"source":["# instead of writing collate_fn function we will use DataCollatorForSeq2Seq\n","# simliarly it implements the batch creation for training\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:04.695838Z","iopub.status.busy":"2023-09-24T15:10:04.695457Z","iopub.status.idle":"2023-09-24T15:10:04.707222Z","shell.execute_reply":"2023-09-24T15:10:04.706315Z","shell.execute_reply.started":"2023-09-24T15:10:04.695806Z"},"id":"UmvbnJ9JIrJd","trusted":true},"outputs":[],"source":["import numpy as np\n","\n","# simple postprocessing for text\n","def postprocess_text(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [[label.strip()] for label in labels]\n","\n","    return preds, labels\n","\n","# compute metrics function to pass to trainer\n","def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","    \n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Some simple post-processing\n","    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n","    result = {\"bleu\": result[\"score\"]}\n","\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    result = {k: round(v, 4) for k, v in result.items()}\n","    return result"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:04.709249Z","iopub.status.busy":"2023-09-24T15:10:04.708526Z","iopub.status.idle":"2023-09-24T15:10:04.806768Z","shell.execute_reply":"2023-09-24T15:10:04.805816Z","shell.execute_reply.started":"2023-09-24T15:10:04.709216Z"},"id":"imY1oC3SIrJf","trusted":true},"outputs":[],"source":["# instead of writing train loop we will use Seq2SeqTrainer\n","trainer = Seq2SeqTrainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:04.808506Z","iopub.status.busy":"2023-09-24T15:10:04.808050Z","iopub.status.idle":"2023-09-24T15:18:44.110261Z","shell.execute_reply":"2023-09-24T15:18:44.109150Z","shell.execute_reply.started":"2023-09-24T15:10:04.808459Z"},"id":"uNx5pyRlIrJh","outputId":"077e661e-d36c-469b-89b8-7ff7f73541ec","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='790' max='790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [790/790 08:37, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Bleu</th>\n","      <th>Gen Len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>1.321529</td>\n","      <td>11.317800</td>\n","      <td>17.656000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>1.318983</td>\n","      <td>11.436100</td>\n","      <td>17.628000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>1.318480</td>\n","      <td>11.293400</td>\n","      <td>17.628000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>1.318595</td>\n","      <td>11.334700</td>\n","      <td>17.634000</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>No log</td>\n","      <td>1.318590</td>\n","      <td>11.361100</td>\n","      <td>17.626000</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>No log</td>\n","      <td>1.318964</td>\n","      <td>11.259700</td>\n","      <td>17.624000</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>1.301300</td>\n","      <td>1.319077</td>\n","      <td>11.268900</td>\n","      <td>17.622000</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>1.301300</td>\n","      <td>1.319275</td>\n","      <td>11.263800</td>\n","      <td>17.626000</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>1.301300</td>\n","      <td>1.319407</td>\n","      <td>11.285900</td>\n","      <td>17.618000</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>1.301300</td>\n","      <td>1.319475</td>\n","      <td>11.276600</td>\n","      <td>17.614000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=790, training_loss=1.289421912084652, metrics={'train_runtime': 518.2799, 'train_samples_per_second': 96.473, 'train_steps_per_second': 1.524, 'total_flos': 1368531887456256.0, 'train_loss': 1.289421912084652, 'epoch': 10.0})"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:19:29.403450Z","iopub.status.busy":"2023-09-24T15:19:29.403061Z","iopub.status.idle":"2023-09-24T15:19:30.003295Z","shell.execute_reply":"2023-09-24T15:19:30.002182Z","shell.execute_reply.started":"2023-09-24T15:19:29.403420Z"},"trusted":true},"outputs":[],"source":["# saving model\n","trainer.save_model('best')"]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:19:30.753608Z","iopub.status.busy":"2023-09-24T15:19:30.753167Z","iopub.status.idle":"2023-09-24T15:19:31.676057Z","shell.execute_reply":"2023-09-24T15:19:31.675005Z","shell.execute_reply.started":"2023-09-24T15:19:30.753575Z"},"trusted":true},"outputs":[],"source":["# loading the model and run inference for it\n","model = AutoModelForSeq2SeqLM.from_pretrained('best')\n","model.eval()\n","model.config.use_cache = False"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:19:31.744595Z","iopub.status.busy":"2023-09-24T15:19:31.744257Z","iopub.status.idle":"2023-09-24T15:19:31.749935Z","shell.execute_reply":"2023-09-24T15:19:31.748926Z","shell.execute_reply.started":"2023-09-24T15:19:31.744568Z"},"trusted":true},"outputs":[],"source":["def translate(model, inference_request, tokenizer=tokenizer):\n","    input_ids = tokenizer(inference_request, return_tensors=\"pt\").input_ids\n","    outputs = model.generate(input_ids=input_ids)\n","    print(tokenizer.decode(outputs[0], skip_special_tokens=True,temperature=0))"]},{"cell_type":"code","execution_count":88,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T16:13:02.462385Z","iopub.status.busy":"2023-09-24T16:13:02.461389Z","iopub.status.idle":"2023-09-24T16:13:02.943140Z","shell.execute_reply":"2023-09-24T16:13:02.941981Z","shell.execute_reply.started":"2023-09-24T16:13:02.462348Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Warum hat es so lange dauern, das Modell zu trainieren?\n"]}],"source":["inference_request = prefix + 'Why did it take so long to train the model?'\n","translate(model, inference_request,tokenizer)"]},{"cell_type":"code","execution_count":89,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T16:13:10.223355Z","iopub.status.busy":"2023-09-24T16:13:10.222968Z","iopub.status.idle":"2023-09-24T16:13:10.549431Z","shell.execute_reply":"2023-09-24T16:13:10.548426Z","shell.execute_reply.started":"2023-09-24T16:13:10.223326Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Mein Name ist Wolfgang und ich lebe in Berlin.\n"]}],"source":["inference_request = prefix +\"My name is Wolfgang and I live in Berlin\"\n","translate(model, inference_request,tokenizer)"]},{"cell_type":"code","execution_count":92,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T16:17:24.580846Z","iopub.status.busy":"2023-09-24T16:17:24.580402Z","iopub.status.idle":"2023-09-24T16:17:24.852369Z","shell.execute_reply":"2023-09-24T16:17:24.851186Z","shell.execute_reply.started":"2023-09-24T16:17:24.580812Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Ihre Aufgabe ist schwierig, beginnen Sie sie heute\n"]}],"source":["inference_request = prefix + \"Your assignment is hard. Start it today\"\n","translate(model, inference_request,tokenizer)"]}],"metadata":{"colab":{"name":"Translation","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
